{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "torch.backends.cudnn.benchmark = True\n",
    "\n",
    "import os\n",
    "import cv2\n",
    "import shutil\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import pretrainedmodels\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from apex import amp\n",
    "from time import time\n",
    "from warnings import filterwarnings\n",
    "from tqdm.notebook import tqdm\n",
    "from efficientnet_pytorch import EfficientNet\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import argparse\n",
    "\n",
    "import gc\n",
    "from PIL import Image\n",
    "from albumentations import *\n",
    "from albumentations.pytorch import ToTensorV2\n",
    "from torchvision.transforms import ToPILImage\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "\n",
    "torch.backends.cudnn.benchmark = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sample merging script for SWA\n",
    "train_root = 'data/train_384'\n",
    "test_root = 'data/test_images'\n",
    "model_dir = 'experiments/final_b4_0_stage1'\n",
    "model_names = ['4.pt', '3.pt', '2.pt', '1.pt', '0.pt']\n",
    "batch_size = 128\n",
    "image_size = 384\n",
    "device = 'cuda:0'\n",
    "fold = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_transform = Compose([\n",
    "    Resize(image_size, image_size, interpolation=cv2.INTER_LANCZOS4),\n",
    "    Normalize(),\n",
    "    ToTensorV2(),\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TrashDataset_test(Dataset):\n",
    "    def __init__(self, df, root='data', transform=None):\n",
    "        self.transform = transform\n",
    "        self.df = df\n",
    "        self.root = root\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.df)\n",
    "\n",
    "    def __getitem__(self,idx):\n",
    "        path = os.path.join(self.root, str(self.df['id'].values[idx])+'.png')\n",
    "        # try:\n",
    "        image = np.array(Image.open(path))\n",
    "        if self.transform is not None:\n",
    "            image = self.transform(image=image)['image']\n",
    "        return {'img':image}\n",
    "\n",
    "class TrashDataset(Dataset):\n",
    "    def __init__(self, df, root, transforms=ToTensorV2()):\n",
    "        self.df, self.root = df, root\n",
    "        self.len = len(df)\n",
    "        self.transforms = transforms\n",
    "    \n",
    "    def __len__(self):\n",
    "        return self.len\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        name, label = str(self.df.image_id.values[idx]), self.df.label.values[idx]\n",
    "        img = cv2.imread(os.path.join(self.root, name+'.png'))\n",
    "        original = img.copy()\n",
    "        img = self.transforms(image=img)['image']\n",
    "        return {'img':img, 'label':label, 'original':original}\n",
    "\n",
    "def metric(preds, labels):\n",
    "    preds = preds.argmax(1)\n",
    "    return (preds == labels).float().mean().item()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "valset = TrashDataset(pd.read_csv('data/train.csv'), train_root, test_transform)\n",
    "indices = np.array(range(len(valset)))\n",
    "skf = StratifiedKFold(n_splits=5, shuffle=True, random_state=2020)\n",
    "for i, (train_indices, val_indices) in enumerate(skf.split(indices, valset.df.label.values)):\n",
    "    if i == fold:\n",
    "        break\n",
    "valset = torch.utils.data.Subset(valset, val_indices)\n",
    "testset = TrashDataset_test(pd.read_csv('data/test.csv'), test_root, test_transform)\n",
    "print(len(valset), 'validation', len(testset), 'test')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "train_path = 'data/train.json'\n",
    "id_map = {}\n",
    "with open(train_path, 'r') as fp:\n",
    "    train = dict(json.load(fp))\n",
    "for instance in train['categories']:\n",
    "    id_map[instance['id']] = instance['name']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load a base model\n",
    "model = torch.jit.load(os.path.join(model_dir, model_names[0])).to(device)\n",
    "model.eval()\n",
    "valloader = DataLoader(valset, batch_size=batch_size, num_workers=8)\n",
    "testloader = DataLoader(testset, batch_size=batch_size, num_workers=8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(model, state_dict, demonstrate=False, half=False):\n",
    "    model.load_state_dict(state_dict)\n",
    "    if half:\n",
    "        model = model.half()\n",
    "    else:\n",
    "        model = model.float()\n",
    "    metric_values, preds_, pred_values_, labels_ = [], [], [], []\n",
    "    with torch.no_grad():\n",
    "        i0 = tqdm(valloader)\n",
    "        for batch in i0:\n",
    "            img, labels, original = batch['img'].to(device), batch['label'].to(device), batch['original']\n",
    "            if half:\n",
    "                img = img.half()\n",
    "            p0 = F.softmax(model(img), dim=1)\n",
    "            p1 = F.softmax(model(torch.flip(img, (-1,))), dim=1)\n",
    "            p2 = F.softmax(model(torch.flip(img, (-2,))), dim=1)\n",
    "            p3 = F.softmax(model(torch.flip(img, (-1, -2))), dim=1)\n",
    "            preds = ((p0 + p1 + p2 + p3) / 4).float()\n",
    "\n",
    "            pred_values, preds = preds.max(1)\n",
    "            preds_.append(preds.cpu())\n",
    "            pred_values_.append(pred_values.cpu())\n",
    "            labels_.append(labels.cpu())\n",
    "            wrong_mask = preds != labels\n",
    "            wrong_ids = torch.tensor(range(len(img)))[wrong_mask]\n",
    "            if len(wrong_ids) > 0 and demonstrate:\n",
    "                for wrong_id in wrong_ids:\n",
    "                    label = id_map[labels[wrong_id].item()+1]\n",
    "                    pred = id_map[preds[wrong_id].item()+1]\n",
    "                    confidence = pred_values[wrong_id].item()\n",
    "                    print('Pred:', pred, ' label:', label, ' conf:', round(confidence, 4))\n",
    "                    wrong_img = original[wrong_id]\n",
    "                    plt.imshow(wrong_img)\n",
    "                    plt.show()\n",
    "            metric_value = 1 - wrong_mask.float().mean().item()\n",
    "            metric_values.append(metric_value)\n",
    "            i0.set_postfix({'Metric': np.mean(metric_values)})\n",
    "    if demonstrate:\n",
    "        print('Metric:', np.mean(metric_values))\n",
    "        return np.mean(metric_values), torch.cat(preds_), torch.cat(pred_values_), torch.cat(labels_)\n",
    "    else:\n",
    "        return np.mean(metric_values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import OrderedDict \n",
    "\n",
    "# Average model weights\n",
    "def avg_state_dict(weights):\n",
    "    average_dict = OrderedDict()\n",
    "    for k in weights[0].keys():\n",
    "        average_dict[k] = sum([weight[k] for weight in weights]) / len(weights)\n",
    "    return average_dict "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "weights = [torch.jit.load(os.path.join(model_dir, model_name)).state_dict() for model_name in model_names]\n",
    "current_weights, metrics = [], []\n",
    "best_weight, patience = None, 0\n",
    "for w in weights:\n",
    "    current_weights.append(w)\n",
    "    average_weights = avg_state_dict(current_weights)\n",
    "    metrics.append(evaluate(model, average_weights, half=True))\n",
    "    if metrics[-1] == max(metrics):\n",
    "        print('Better combo found with CV:', metrics[-1])\n",
    "        best_weight = average_weights\n",
    "        patience = 0\n",
    "    else:\n",
    "        patience += 1\n",
    "        if patience == 10:\n",
    "            print('Early stopping triggered ;)')\n",
    "            break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.load_state_dict(best_weight)\n",
    "model.eval()\n",
    "torch.jit.save(model, os.path.join(model_dir, 'merged_best.pt'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(model_dir)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.7.4 64-bit ('base': conda)",
   "language": "python",
   "name": "python37464bitbaseconda4dcd48f05ad640a193e05f9debc114ce"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
